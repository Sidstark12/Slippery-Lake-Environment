{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidstark12/Soc-intro-to-Machine-Intelligence/blob/SOC-Q-Learning/SoC_final_project_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLkgaku5DHtF"
      },
      "source": [
        "## Scenario - Slippery lake Environment\n",
        "\n",
        "Build an AI agent from scratch, capable of traversing a predetermined environment, despite the inherent stochasticity.\n",
        "The final goal is to maximise our chances of reaching the final goal in the minimum number fo steps witout touhcing any pitfalls intiating from the white tiles.\n",
        "We will use Q-Learning to accomplish this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ofRcCz7DHtM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAEu6MolDHtO"
      },
      "source": [
        "## Defining Environment\n",
        "\n",
        "### States\n",
        "The states in the environment are all of the possible places we can move through, these are (*grey squares*) pit falls, moving tiles (*white squares*),etc.The final goal is the *green square*.\n",
        "\n",
        "* Grey and green are the terminal states.\n",
        "* Orange squares are super slippery surface.\n",
        "* Blue squares are non-slippery surface.\n",
        "* White squares are slightly slippery surface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9MYFTmPDHtP"
      },
      "outputs": [],
      "source": [
        "#define the shape of the environment (i.e., its states)\n",
        "envi_rows = 7\n",
        "envi_columns = 7\n",
        "\n",
        "# A 3D array is created to take in account of environment as rows and columns, actions of the agent.\n",
        "#\"Action\" dimension consists of 4 layers\n",
        "#The value of each (state, action) pair is initialized to 0.\n",
        "q_values = np.zeros((envi_rows, envi_columns, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGrmOFEuDHtQ"
      },
      "source": [
        "### Actions\n",
        "The actions availabel for the AI agent are to move in the four directions:\n",
        "* North\n",
        "* East\n",
        "* West\n",
        "* South"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQeirTt-DHtR"
      },
      "outputs": [],
      "source": [
        "#numeric action codes:0 = North, 1 = East, 2 = West, 3 = South.\n",
        "actions = ['North','East','West','South']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DHdG9sHDHtR"
      },
      "source": [
        "### Rewards\n",
        "Negative rewards (i.e., punishments) are used for all states except the goal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6CSK03HDHtT",
        "outputId": "83b0e739-33c4-4226-9a90-626156ddf35a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[  -1.   -1.   -1. -100.   -1.   -1.   -1.]\n",
            "[  -1. -100.   50.   50.   50. -100.   -1.]\n",
            "[-50.  -1.  -1.  -1.  -1.  -1.  -1.]\n",
            "[  -1.   -1. -100.  100. -100.   -1. -100.]\n",
            "[  -1.   -1.   50. -100.   -1.   -1.   -1.]\n",
            "[  -1. -100.   -1.   -1.   50.  -50.   -1.]\n",
            "[  50.   -1.   -1.   -1.   -1. -100.   -1.]\n"
          ]
        }
      ],
      "source": [
        "# Create a 2D numpy array to hold the reward for each state.\n",
        "rewards = np.full((envi_rows,envi_columns),-1.)\n",
        "rewards[3,3] = 100\n",
        "\n",
        "#value for non-slippery surface = -25\n",
        "#value for super slippery surface = -50\n",
        "#now defining the aile values\n",
        "aisle = {} #locations are being stored in dictionary\n",
        "aisle[0] = [3]\n",
        "aisle[1] = [1,5]\n",
        "aisle[2] = []\n",
        "aisle[3] = [2,4,6]\n",
        "aisle[4] = [3]\n",
        "aisle[5] = [1]\n",
        "aisle[6] = [5]\n",
        "\n",
        "#set the rewards for the aisle locations (grey squares)\n",
        "for row_index in range(0,7):\n",
        "    for column_index in aisle[row_index]:\n",
        "        rewards[row_index,column_index] = -100\n",
        "\n",
        "# now giving the values of blue sqaures\n",
        "aisle_blue = {}\n",
        "aisle_blue[0]= []\n",
        "aisle_blue[1]=[2,3,4]\n",
        "aisle_blue[2]=[]\n",
        "aisle_blue[3]=[]\n",
        "aisle_blue[4]=[2]\n",
        "aisle_blue[5]=[4]\n",
        "aisle_blue[6]=[0]\n",
        "\n",
        "for row_index in range(1,7):\n",
        "    for column_index in aisle_blue[row_index]:\n",
        "        rewards[row_index,column_index] = 50\n",
        "        \n",
        "# now giving values to orange sqaures\n",
        "aisle_orange = {}\n",
        "aisle_orange[0]=[]\n",
        "aisle_orange[1]=[]\n",
        "aisle_orange[2]=[0]\n",
        "aisle_orange[3]=[]\n",
        "aisle_orange[4]=[]\n",
        "aisle_orange[5]=[5]\n",
        "aisle_orange[6]=[]\n",
        "\n",
        "for row_index in range(1,7):\n",
        "    for column_index in aisle_orange[row_index]:\n",
        "        rewards[row_index,column_index] = -50\n",
        "#print the matrix\n",
        "for row in rewards:\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqIgVEMiDHtV"
      },
      "source": [
        "Now we have our envieronment ready, hence we'll proceed to making of helper functions for our agent. Some points that we need to attend are\n",
        "* According to the given condtion the intial postion of starting should be a white sqaure.\n",
        "* The actions of the agent will be selected using the *epsilon greedy algorithm*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhXgduQIDHtW"
      },
      "outputs": [],
      "source": [
        "# Now we define a function that will determine if the location selected is terminal state or not\n",
        "# we'll determine the position on the basis of reward given to it\n",
        "\n",
        "# this functions determines if the surface is a terminal state or not\n",
        "def argmax(q_values):\n",
        "    top = float(\"-inf\")\n",
        "    ties = []\n",
        "    for i in range(len(q_values)):\n",
        "        if q_values[i] > top:\n",
        "            top, ties = q_values[i], [i]\n",
        "        elif q_values[i] == top:\n",
        "            ties.append(i)\n",
        "    ind = np.random.choice(ties)\n",
        "    return ind\n",
        "\n",
        "\n",
        "def terminal_state(current_row_index,current_column_index):\n",
        "    if rewards[current_row_index,current_column_index] == -100 or rewards[current_row_index,current_column_index] == 100:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "# till now we are able to determine if the surface is a terminal state or not\n",
        "\n",
        "# now select a random, non terminal starting state\n",
        "def get_starting_location():\n",
        "    current_row_index = np.random.randint(envi_rows)\n",
        "    current_column_index = np.random.randint(envi_columns)\n",
        "    while terminal_state(current_row_index,current_column_index):\n",
        "        current_row_index = np.random.randint(envi_rows)\n",
        "        current_column_index = np.random.randint(envi_columns)\n",
        "    return current_row_index, current_column_index\n",
        "\n",
        "# till now our random white tile starting surface is selected \n",
        "\n",
        "# now we define epsilon greedy algorithm to determine the action the agent needs to take\n",
        "\n",
        "def get_next_action(current_row_index,current_column_index):\n",
        "    action_index = argmax(q_values[current_row_index, current_column_index])\n",
        "    return action_index\n",
        "\n",
        "    \n",
        "\n",
        "# this fucntion will get the next location \n",
        "def get_next_location(current_row_index,current_column_index,action_index):\n",
        "    new_row_index = current_row_index\n",
        "    new_column_index = current_column_index\n",
        "    if actions[action_index] == 'North' and current_row_index >0:\n",
        "        new_row_index -= 1\n",
        "    elif actions[action_index] == 'East' and current_column_index < envi_columns - 1:\n",
        "        new_column_index += 1\n",
        "    elif actions[action_index] == 'West' and current_column_index >0:\n",
        "        new_column_index -= 1 \n",
        "    elif actions[action_index] == 'South' and current_row_index < envi_rows - 1:\n",
        "        new_row_index += 1\n",
        "    return new_row_index, new_column_index  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "or55GJXVDHtX",
        "outputId": "a38ec527-c214-4c06-bc21-af28faa3f001"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0\n",
            "Episode: 1000\n",
            "Episode: 2000\n",
            "Episode: 3000\n",
            "Episode: 4000\n",
            "Episode: 5000\n",
            "Episode: 6000\n",
            "Episode: 7000\n",
            "Episode: 8000\n",
            "Episode: 9000\n",
            "Episode: 10000\n",
            "Episode: 11000\n",
            "Episode: 12000\n",
            "Episode: 13000\n",
            "Episode: 14000\n",
            "Episode: 15000\n",
            "Episode: 16000\n",
            "Episode: 17000\n",
            "Episode: 18000\n",
            "Episode: 19000\n",
            "Episode: 20000\n",
            "Episode: 21000\n",
            "Episode: 22000\n",
            "Episode: 23000\n",
            "Episode: 24000\n",
            "Episode: 25000\n",
            "Episode: 26000\n",
            "Episode: 27000\n",
            "Episode: 28000\n",
            "Episode: 29000\n",
            "Episode: 30000\n",
            "Episode: 31000\n",
            "Episode: 32000\n",
            "Episode: 33000\n",
            "Episode: 34000\n",
            "Episode: 35000\n",
            "Episode: 36000\n",
            "Episode: 37000\n",
            "Episode: 38000\n",
            "Episode: 39000\n",
            "Episode: 40000\n",
            "Episode: 41000\n",
            "Episode: 42000\n",
            "Episode: 43000\n",
            "Episode: 44000\n",
            "Episode: 45000\n",
            "Episode: 46000\n",
            "Episode: 47000\n",
            "Episode: 48000\n",
            "Episode: 49000\n",
            "Episode: 50000\n",
            "Episode: 51000\n",
            "Episode: 52000\n",
            "Episode: 53000\n",
            "Episode: 54000\n",
            "Episode: 55000\n",
            "Episode: 56000\n",
            "Episode: 57000\n",
            "Episode: 58000\n",
            "Episode: 59000\n",
            "Episode: 60000\n",
            "Episode: 61000\n",
            "Episode: 62000\n",
            "Episode: 63000\n",
            "Episode: 64000\n",
            "Episode: 65000\n",
            "Episode: 66000\n",
            "Episode: 67000\n",
            "Episode: 68000\n",
            "Episode: 69000\n",
            "Episode: 70000\n",
            "Episode: 71000\n",
            "Episode: 72000\n",
            "Episode: 73000\n",
            "Episode: 74000\n",
            "Episode: 75000\n",
            "Episode: 76000\n",
            "Episode: 77000\n",
            "Episode: 78000\n",
            "Episode: 79000\n",
            "Episode: 80000\n",
            "Episode: 81000\n",
            "Episode: 82000\n",
            "Episode: 83000\n",
            "Episode: 84000\n",
            "Episode: 85000\n",
            "Episode: 86000\n",
            "Episode: 87000\n",
            "Episode: 88000\n",
            "Episode: 89000\n",
            "Episode: 90000\n",
            "Episode: 91000\n",
            "Episode: 92000\n",
            "Episode: 93000\n",
            "Episode: 94000\n",
            "Episode: 95000\n",
            "Episode: 96000\n",
            "Episode: 97000\n",
            "Episode: 98000\n",
            "Episode: 99000\n",
            "Episode: 100000\n",
            "training complete\n"
          ]
        }
      ],
      "source": [
        "# training parameters\n",
        "alpha = 0.1 # learning rate\n",
        "gamma = 0.99\n",
        "epsilon = 0.1\n",
        "\n",
        "# run through 15500 episodes\n",
        "for episode in range(100001):\n",
        "    row_index, column_index = get_starting_location()\n",
        "    while not terminal_state(row_index, column_index):\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action_index = np.random.choice([0,1,2,3]) \n",
        "        else:\n",
        "            action_index = get_next_action(row_index, column_index)\n",
        "        \n",
        "        new_row_index, new_column_index = get_next_location(row_index,column_index,action_index)\n",
        "        reward_new = rewards[new_row_index, new_column_index]\n",
        "        \n",
        "        old_value = q_values[row_index,column_index,action_index]\n",
        "        next_max = np.max(q_values[new_row_index,new_column_index])\n",
        "        \n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward_new + gamma * next_max)\n",
        "        q_values[row_index,column_index,action_index] = new_value\n",
        "        \n",
        "        row_index = new_row_index\n",
        "        column_index = new_column_index\n",
        "    if episode % 1000 == 0:\n",
        "        print(f\"Episode: {episode}\")        \n",
        "print('training complete')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3L9H58TDHtY"
      },
      "outputs": [],
      "source": [
        "# this function will select the shortest path from the current location to the final position\n",
        "# it help in moving the agent\n",
        "def get_shortest_path(start_row_index,start_column_index):\n",
        "    shortest_path = []\n",
        "    i = 0\n",
        "    if terminal_state(start_row_index,start_column_index):\n",
        "        print (\"terminal state\")\n",
        "    else:\n",
        "        current_row_index,current_column_index = start_row_index,start_column_index\n",
        "        while not terminal_state(current_row_index,current_column_index):\n",
        "            i = i+1\n",
        "            if i>100:\n",
        "                break\n",
        "            action_index = get_next_action(current_row_index, current_column_index)\n",
        "            current_row_index,current_column_index = get_next_location(current_row_index,current_column_index,action_index)\n",
        "            shortest_path.append(actions[action_index])\n",
        "    return shortest_path   \n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "so9MURLBDHtZ",
        "outputId": "e1340fc5-8b0f-4595-b1aa-7daddd8fa7a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start state 0, 5\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['West',\n",
              " 'South',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'West',\n",
              " 'East',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'West',\n",
              " 'East',\n",
              " 'East',\n",
              " 'West',\n",
              " 'West',\n",
              " 'East',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'East',\n",
              " 'West',\n",
              " 'West',\n",
              " 'East',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'West',\n",
              " 'East',\n",
              " 'East',\n",
              " 'West',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'West',\n",
              " 'East',\n",
              " 'East',\n",
              " 'West',\n",
              " 'West',\n",
              " 'East',\n",
              " 'East',\n",
              " 'West',\n",
              " 'West',\n",
              " 'East',\n",
              " 'East',\n",
              " 'West',\n",
              " 'West',\n",
              " 'East',\n",
              " 'East',\n",
              " 'West',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'East',\n",
              " 'West',\n",
              " 'West',\n",
              " 'East',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East',\n",
              " 'West',\n",
              " 'East']"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "start_row_index, start_column_index = get_starting_location()\n",
        "# start_row_index, start_column_index = 1,4\n",
        "print(f\"start state {start_row_index}, {start_column_index}\")\n",
        "shortest_path = get_shortest_path(start_row_index,start_column_index)\n",
        "shortest_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Eb1lme8DHtZ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "SoC final project .ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
